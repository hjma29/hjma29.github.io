{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"whitepaper/","title":"Monitoring HPE GreenLake Servers running GPU using Grafana and Prometheus","text":""},{"location":"whitepaper/#overview","title":"Overview","text":"<p>HPE GreenLake provides a cloud-native platform for managing and monitoring infrastructure with built-in tools and dashboards. While GreenLake offers comprehensive native monitoring capabilities, organizations can also leverage the GreenLake API to integrate with popular open-source tools like Grafana and Prometheus. This approach enables teams to consolidate monitoring data across hybrid environments, utilize existing observability workflows, and create customized dashboards tailored to specific operational needs.</p>"},{"location":"whitepaper/#kubernetes-and-helm-setup","title":"Kubernetes and Helm Setup","text":""},{"location":"whitepaper/#kubernetes-cluster-setup","title":"Kubernetes cluster setup","text":"<p>This demonstration environment utilizes a high-availability Kubernetes cluster consisting of three control plane nodes and two worker nodes, </p> <pre><code>wsl=&gt; k get node -o wide\nNAME                                STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME\nc2-cp-01.hst.enablement.local       Ready    control-plane   80d   v1.32.5   10.16.160.51   &lt;none&gt;        Ubuntu 22.04.5 LTS   5.15.0-144-generic   containerd://2.0.5\nc2-cp-02.hst.enablement.local       Ready    control-plane   80d   v1.32.5   10.16.160.52   &lt;none&gt;        Ubuntu 22.04.5 LTS   5.15.0-144-generic   containerd://2.0.5\nc2-cp-03.hst.enablement.local       Ready    control-plane   80d   v1.32.5   10.16.160.53   &lt;none&gt;        Ubuntu 22.04.5 LTS   5.15.0-144-generic   containerd://2.0.5\nc2-worker-01.hst.enablement.local   Ready    &lt;none&gt;          80d   v1.32.5   10.16.160.54   &lt;none&gt;        Ubuntu 22.04.5 LTS   5.15.0-144-generic   containerd://2.0.5\nc2-worker-02.hst.enablement.local   Ready    &lt;none&gt;          80d   v1.32.5   10.16.160.55   &lt;none&gt;        Ubuntu 22.04.5 LTS   5.15.0-144-generic   containerd://2.0.5\n</code></pre>"},{"location":"whitepaper/#kubernetes-namespace-setup","title":"Kubernetes namespace setup","text":"<p>The cluster is equipped with the <code>gpu-operator</code> namespace for NVIDIA GPU management and the <code>monitoring</code> namespace hosting the Prometheus stack, with external access enabled via NodePort services. <pre><code>wsl=&gt; kubectl get ns | grep -vE '^(kube-|default)'\nNAME              STATUS   AGE\ngpu-operator      Active   80d\nmonitoring        Active   56d\n\nwsl=&gt; k get svc -n gpu-operator \nNAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\ngpu-operator           ClusterIP   10.233.44.80   &lt;none&gt;        8080/TCP   78d\nnvidia-dcgm-exporter   ClusterIP   10.233.15.59   &lt;none&gt;        9400/TCP   78d\n\nwsl=&gt; k get svc --field-selector spec.type=NodePort -n monitoring\nNAME                               TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE\nkube-prometheus-stack-grafana      NodePort   10.233.22.241   &lt;none&gt;        80:30080/TCP                    56d\nkube-prometheus-stack-prometheus   NodePort   10.233.8.106    &lt;none&gt;        9090:30090/TCP,8080:30398/TCP   56d\n</code></pre></p>"},{"location":"whitepaper/#helm-chart-installation","title":"Helm chart installation","text":"<p>The environment uses Helm to manage two key components: the NVIDIA GPU Operator for GPU resource management and the Kube Prometheus Stack for monitoring and observability.</p> <pre><code>wsl=&gt; helm list -A\nNAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\ngpu-operator-1753140595 gpu-operator    4               2025-08-14 19:20:42.329819669 -0700 MST deployed        gpu-operator-v25.3.2            v25.3.2    \nkube-prometheus-stack   monitoring      5               2025-08-15 13:06:31.169338089 -0700 MST deployed        kube-prometheus-stack-76.3.\n0    v0.84.1    \n</code></pre>"},{"location":"whitepaper/#gpu-operator-chart-customization","title":"GPU Operator chart customization","text":"<p>The NVIDIA GPU Operator Helm chart deploys a DCGM (Data Center GPU Manager) exporter by default, but there are important nuances:</p> <ul> <li> <p>The DCGM exporter Pod will be created automatically when the operator detects a node with an NVIDIA GPU and the dcgm-exporter component is enabled in its values. <pre><code>wsl=&gt; k -n gpu-operator get pods -o wide | grep dcgm\nnvidia-dcgm-exporter-gkg6d                                        1/1     Running     0          56d   10.233.117.209   c2-worker-01.hst.enablement.local   &lt;none&gt;           &lt;none&gt;\nnvidia-dcgm-exporter-r2np6                                        1/1     Running     0          56d   10.233.114.15    c2-worker-02.hst.enablement.local   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> </li> <li> <p>In the stock gpu-operator Helm chart from NVIDIA's repository, the DCGM exporter is enabled by default (<code>dcgmExporter.enabled: true</code>), but the ServiceMonitor is disabled by default (<code>serviceMonitor.enabled: false</code>). See the NVIDIA GPU Operator Documentation.</p> </li> </ul> <p></p> <p>You can also verify these default built-in values using the <code>helm show values</code> command.  <pre><code>wsl=&gt; helm show values nvidia/gpu-operator | grep -A 15 dcgmExporter\ndcgmExporter:\n  enabled: true\n  repository: nvcr.io/nvidia/k8s\n  image: dcgm-exporter\n  version: 4.3.1-4.4.0-ubuntu22.04\n  imagePullPolicy: IfNotPresent\n  env: []\n  resources: {}\n  service:\n    internalTrafficPolicy: Cluster\n  serviceMonitor:\n    enabled: false\n    interval: 15s\n    honorLabels: false\n    additionalLabels: {}\n    relabelings: []\n</code></pre></p> <p>We need to enable  the ServiceMonitor (<code>dcgmExporter.serviceMonitor.enabled: true</code>) in order for Prometheus to automatically scrape the DCGM exporter.</p> <p>The gpu-operator is configured with custom values to enable Prometheus integration. The DCGM exporter runs as a ClusterIP service with ServiceMonitor enabled for automatic metrics discovery by Prometheus. <pre><code>wsl=&gt; helm get values gpu-operator-1753140595 -n gpu-operator\nUSER-SUPPLIED VALUES:\ndcgmExporter:\n  service:\n    type: ClusterIP\n  serviceMonitor:\n    enabled: true\n</code></pre></p>"},{"location":"whitepaper/#gpu-utilization-simulation","title":"GPU utilization simulation","text":"<p>To simulate GPU load and verify monitoring functionality, we deployed a test pod running the gpu-burn utility. This tool performs intensive GPU computations, allowing us to observe GPU utilization metrics in our monitoring dashboards.</p> <p>The following YAML manifest creates a pod that clones the gpu-burn repository, compiles it, and runs continuous GPU stress testing:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: gpu-burn\nspec:\n    containers:\n        - name: gpu-burn\n            image: nvidia/cuda:12.2.0-devel-ubuntu22.04 \n            command: [\"/bin/bash\", \"-c\"]\n            args:\n                - |\n                    apt update &amp;&amp; apt install -y git build-essential &amp;&amp; \\\n                    git clone https://github.com/wilicc/gpu-burn.git &amp;&amp; \\\n                    cd gpu-burn &amp;&amp; make &amp;&amp; ./gpu_burn 999999 \n            resources:\n                limits:\n                    nvidia.com/gpu: 1\n    restartPolicy: Never\n</code></pre> <p>Key configuration details: - Base image: <code>nvidia/cuda:12.2.0-devel-ubuntu22.04</code> provides the CUDA development environment - GPU allocation: <code>nvidia.com/gpu: 1</code> requests a single GPU from the cluster - Runtime: <code>gpu_burn 999999</code> runs for approximately 277 hours (effectively continuous) - Restart policy: <code>Never</code> ensures the pod completes its run without automatic restarts  </p> <p>Deploy the pod using: <pre><code>kubectl apply -f gpu-burn.yaml\n</code></pre></p>"},{"location":"whitepaper/#grafana-cloud-integration","title":"Grafana Cloud Integration","text":"<p>While the local Grafana deployment provides comprehensive monitoring capabilities, organizations often need to share dashboards with team members who cannot directly access the internal infrastructure. Grafana Cloud offers an ideal solution by enabling metrics to be pushed from the local Prometheus instance to a cloud-hosted environment, making dashboards accessible to remote teams without requiring VPN or direct network access.</p>"},{"location":"whitepaper/#prometheus-remote-write","title":"Prometheus Remote Write","text":"<p>Grafana Cloud supports Prometheus remote write protocol, allowing local Prometheus to continuously push metrics to the cloud. This approach offers several advantages:</p> <ul> <li>No inbound firewall rules required - Metrics are pushed outbound from the lab</li> <li>Real-time data synchronization - Metrics appear in Grafana Cloud within seconds</li> <li>Selective metric filtering - Control which metrics are sent to manage costs</li> <li>Multi-cluster aggregation - Consolidate metrics from multiple environments</li> </ul> <p>The integration involves two main steps:</p> <ol> <li>Configure Grafana Cloud credentials - Create a Prometheus remote write endpoint and API key in Grafana Cloud</li> <li>Update Prometheus configuration - Add remote write settings to the kube-prometheus-stack Helm values</li> </ol>"},{"location":"whitepaper/#prometheus-remote-write-diagram","title":"Prometheus Remote Write Diagram","text":"<p>The following diagram from Grafana Cloud documentation shows the architecture of Prometheous remote write. </p> <p>How it works: 1. Prometheus scrapes metrics from all targets (DCGM exporter, kube-state-metrics, etc.) 2. Stores metrics locally in TSDB (Time Series Database) 3. Simultaneously pushes metrics to Grafana Cloud via remote write 4. Local Grafana and Prometheus UI can query local data 5. Grafana Cloud receives a copy of all metrics</p>"},{"location":"whitepaper/#configure-grafana-cloud-credentials","title":"Configure Grafana Cloud credentials","text":"<p>Users should login to <code>grafana.com</code> to access their grafana stack and retrive the prometheous cloud instance API endpoint by selecting \"Details\".  </p> <p>Select \"Prometheous Details\" </p> <p>In <code>Prometheous Instance Details</code> page, please note \"Remote Write Endpoint\", \"Instance ID\" and generate \"API Token\" </p>"},{"location":"whitepaper/#updating-helm-release-with-configured-value","title":"Updating Helm release with configured value","text":"<ol> <li> <p>Create Kubernetes secret to store the username and password to access prometheous cloud instance <pre><code>wsl=&gt; GRAFANA_CLOUD_USER=\"&lt;your userid here&gt;\"\nwsl=&gt; GRAFANA_CLOUD_PASSWORD=\"&lt;you token here&gt;\"\n\nwsl=&gt; kubectl create secret generic grafana-cloud-credentials \\\n  --from-literal=username=\"${GRAFANA_CLOUD_USER}\" \\\n  --from-literal=password=\"${GRAFANA_CLOUD_PASSWORD}\" \\\n  -n monitoring\nsecret/grafana-cloud-credentials created\n</code></pre></p> </li> <li> <p>Create helm custom values yaml file <pre><code>wsl=&gt; cat prometheus-remote-write-values.yaml \nprometheus:\n  prometheusSpec:\n    remoteWrite:\n      - url: https://&lt;your-prometheus-id-here&gt;.grafana.net/api/prom/push\n        basicAuth:\n          username:\n            name: grafana-cloud-credentials\n            key: username\n          password:\n            name: grafana-cloud-credentials\n            key: password\n</code></pre></p> </li> <li> <p>Upgrade helm releases <pre><code>wsl=&gt; helm upgrade kube-prometheus-stack prometheus-community/kube-prometheus-stack   -n monitoring   -f prometheus-remote-write-values.yaml   --reuse-values\n</code></pre></p> </li> </ol>"},{"location":"whitepaper/#proliant-server-nvidia-gpu-utilization-dashboard","title":"Proliant Server Nvidia GPU utilization dashboard","text":"<p>The following screen shows the Grafana Cloud dashboard of HPE Proliant Server Nvidia GPU Utilization. </p> <p>The corresponding server GPU utilization from <code>nvidia-smi</code> is shown below. <pre><code>wsl=&gt; ssh user01@c2-worker-02.hst.enablement.local nvidia-smi\nMon Oct 20 13:55:11 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA L40S-4C                 On  |   00000000:02:00.0 Off |                    0 |\n| N/A   N/A    P0            N/A  /  N/A  |    3002MiB /   4096MiB |     99%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A         3712514      C   ./gpu_burn                             2997MiB |\n+-----------------------------------------------------------------------------------------+\n</code></pre></p>"}]}