{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"whitepaper/","title":"Monitoring HPE GreenLake Servers running GPU using Grafana and Prometheus","text":""},{"location":"whitepaper/#overview","title":"Overview","text":"<p>HPE GreenLake provides a cloud-native platform for managing and monitoring infrastructure with built-in tools and dashboards. While GreenLake offers comprehensive native monitoring capabilities, organizations can also leverage the GreenLake API to integrate with popular open-source tools like Grafana and Prometheus. This approach enables teams to consolidate monitoring data across hybrid environments, utilize existing observability workflows, and create customized dashboards tailored to specific operational needs.</p>"},{"location":"whitepaper/#kubernetes-and-helm-setup","title":"Kubernetes and Helm Setup","text":""},{"location":"whitepaper/#kubernetes-cluster-setup","title":"Kubernetes cluster setup","text":"<p>This demonstration environment utilizes a high-availability Kubernetes cluster consisting of three control plane nodes and two worker nodes, </p> <pre><code>wsl=&gt; k get node -o wide\nNAME                                STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME\nc2-cp-01.hst.enablement.local       Ready    control-plane   80d   v1.32.5   10.16.160.51   &lt;none&gt;        Ubuntu 22.04.5 LTS   5.15.0-144-generic   containerd://2.0.5\nc2-cp-02.hst.enablement.local       Ready    control-plane   80d   v1.32.5   10.16.160.52   &lt;none&gt;        Ubuntu 22.04.5 LTS   5.15.0-144-generic   containerd://2.0.5\nc2-cp-03.hst.enablement.local       Ready    control-plane   80d   v1.32.5   10.16.160.53   &lt;none&gt;        Ubuntu 22.04.5 LTS   5.15.0-144-generic   containerd://2.0.5\nc2-worker-01.hst.enablement.local   Ready    &lt;none&gt;          80d   v1.32.5   10.16.160.54   &lt;none&gt;        Ubuntu 22.04.5 LTS   5.15.0-144-generic   containerd://2.0.5\nc2-worker-02.hst.enablement.local   Ready    &lt;none&gt;          80d   v1.32.5   10.16.160.55   &lt;none&gt;        Ubuntu 22.04.5 LTS   5.15.0-144-generic   containerd://2.0.5\n</code></pre>"},{"location":"whitepaper/#kubernetes-namespace-setup","title":"Kubernetes namespace setup","text":"<p>The cluster is equipped with the <code>gpu-operator</code> namespace for NVIDIA GPU management and the <code>monitoring</code> namespace hosting the Prometheus stack, with external access enabled via NodePort services. <pre><code>wsl=&gt; kubectl get ns | grep -vE '^(kube-|default)'\nNAME              STATUS   AGE\ngpu-operator      Active   80d\nmonitoring        Active   56d\n\nwsl=&gt; k get svc -n gpu-operator \nNAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\ngpu-operator           ClusterIP   10.233.44.80   &lt;none&gt;        8080/TCP   78d\nnvidia-dcgm-exporter   ClusterIP   10.233.15.59   &lt;none&gt;        9400/TCP   78d\n\nwsl=&gt; k get svc --field-selector spec.type=NodePort -n monitoring\nNAME                               TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE\nkube-prometheus-stack-grafana      NodePort   10.233.22.241   &lt;none&gt;        80:30080/TCP                    56d\nkube-prometheus-stack-prometheus   NodePort   10.233.8.106    &lt;none&gt;        9090:30090/TCP,8080:30398/TCP   56d\n</code></pre></p>"},{"location":"whitepaper/#helm-chart-installation","title":"Helm chart installation","text":"<p>The environment uses Helm to manage two key components: the NVIDIA GPU Operator for GPU resource management and the Kube Prometheus Stack for monitoring and observability.</p> <pre><code>wsl=&gt; helm list -A\nNAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\ngpu-operator-1753140595 gpu-operator    4               2025-08-14 19:20:42.329819669 -0700 MST deployed        gpu-operator-v25.3.2            v25.3.2    \nkube-prometheus-stack   monitoring      5               2025-08-15 13:06:31.169338089 -0700 MST deployed        kube-prometheus-stack-76.3.\n0    v0.84.1    \n</code></pre>"},{"location":"whitepaper/#gpu-operator-chart-customization","title":"GPU Operator chart customization","text":"<p>The NVIDIA GPU Operator Helm chart deploys a DCGM (Data Center GPU Manager) exporter by default, but there are important nuances:</p> <ul> <li> <p>The DCGM exporter Pod will be created automatically when the operator detects a node with an NVIDIA GPU and the dcgm-exporter component is enabled in its values. <pre><code>wsl=&gt; k -n gpu-operator get pods -o wide | grep dcgm\nnvidia-dcgm-exporter-gkg6d                                        1/1     Running     0          56d   10.233.117.209   c2-worker-01.hst.enablement.local   &lt;none&gt;           &lt;none&gt;\nnvidia-dcgm-exporter-r2np6                                        1/1     Running     0          56d   10.233.114.15    c2-worker-02.hst.enablement.local   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> </li> <li> <p>In the stock gpu-operator Helm chart from NVIDIA's repo, the DCGM exporter is enabled by default (<code>dcgmExporter.enabled: true</code>). See the NVIDIA GPU Operator Documentation.</p> </li> </ul> <p></p> <p>However:</p> <ol> <li>ServiceMonitor is not enabled by default.</li> <li>This means Prometheus won't automatically scrape the DCGM exporter unless you either:<ul> <li>Enable the ServiceMonitor (<code>dcgmExporter.serviceMonitor.enabled: true</code>), or</li> <li>Manually define a scrape config in Prometheus.</li> </ul> </li> </ol> <p>The gpu-operator is configured with custom values to enable Prometheus integration. The DCGM exporter runs as a ClusterIP service with ServiceMonitor enabled for automatic metrics discovery by Prometheus. <pre><code>wsl=&gt; helm get values gpu-operator-1753140595 -n gpu-operator\nUSER-SUPPLIED VALUES:\ndcgmExporter:\n  service:\n    type: ClusterIP\n  serviceMonitor:\n    enabled: true\n</code></pre></p>"},{"location":"whitepaper/#gpu-utilization-simulation","title":"GPU utilization simulation","text":"<p>To simulate GPU load and verify monitoring functionality, we deployed a test pod running the gpu-burn utility. This tool performs intensive GPU computations, allowing us to observe GPU utilization metrics in our monitoring dashboards.</p> <p>The following YAML manifest creates a pod that clones the gpu-burn repository, compiles it, and runs continuous GPU stress testing:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: gpu-burn\nspec:\n    containers:\n        - name: gpu-burn\n            image: nvidia/cuda:12.2.0-devel-ubuntu22.04 \n            command: [\"/bin/bash\", \"-c\"]\n            args:\n                - |\n                    apt update &amp;&amp; apt install -y git build-essential &amp;&amp; \\\n                    git clone https://github.com/wilicc/gpu-burn.git &amp;&amp; \\\n                    cd gpu-burn &amp;&amp; make &amp;&amp; ./gpu_burn 999999 \n            resources:\n                limits:\n                    nvidia.com/gpu: 1\n    restartPolicy: Never\n</code></pre> <p>Key configuration details: - Base image: <code>nvidia/cuda:12.2.0-devel-ubuntu22.04</code> provides the CUDA development environment - GPU allocation: <code>nvidia.com/gpu: 1</code> requests a single GPU from the cluster - Runtime: <code>gpu_burn 999999</code> runs for approximately 277 hours (effectively continuous) - Restart policy: <code>Never</code> ensures the pod completes its run without automatic restarts  </p> <p>Deploy the pod using: <pre><code>kubectl apply -f gpu-burn.yaml\n</code></pre></p>"}]}